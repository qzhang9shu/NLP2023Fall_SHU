{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"sdBjRnWqLwWP"},"source":["# Inference notenook for [CLIP prefix captioning](https://github.com/rmokady/CLIP_prefix_caption/)\n","\n","Disclaimer: the authors do not own any rights for the code or data."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GRfpGaz27IWs","outputId":"0029ac17-e621-4620-d61f-cab47cfac457","executionInfo":{"status":"ok","timestamp":1698231820121,"user_tz":-480,"elapsed":20006,"user":{"displayName":"ziheng jiang","userId":"11981944010465529762"}}},"source":["#@title Install\n","!pip install transformers\n","! pip install git+https://github.com/openai/CLIP.git\n"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.34.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.15,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.14.1)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Collecting git+https://github.com/openai/CLIP.git\n","  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-_c50tb4u\n","  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-_c50tb4u\n","  Resolved https://github.com/openai/CLIP.git to commit a1d071733d7111c9c014f024669f959182114e33\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: ftfy in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (6.1.1)\n","Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2023.6.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (4.66.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (2.1.0+cu118)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from clip==1.0) (0.16.0+cu118)\n","Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy->clip==1.0) (0.2.8)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.12.4)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->clip==1.0) (2.1.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (1.23.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (2.31.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->clip==1.0) (9.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->clip==1.0) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.3.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision->clip==1.0) (2023.7.22)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->clip==1.0) (1.3.0)\n"]}]},{"cell_type":"code","metadata":{"id":"iqE3Fj5-uYSR","executionInfo":{"status":"ok","timestamp":1698231820629,"user_tz":-480,"elapsed":516,"user":{"displayName":"ziheng jiang","userId":"11981944010465529762"}}},"source":["#@title Drive Downloader\n","\n","from pydrive.auth import GoogleAuth\n","from pydrive.drive import GoogleDrive\n","from google.colab import auth\n","from oauth2client.client import GoogleCredentials\n","\n","download_with_pydrive = True #@param {type:\"boolean\"}\n","\n","class Downloader(object):\n","    def __init__(self, use_pydrive):\n","        self.use_pydrive = use_pydrive\n","\n","        if self.use_pydrive:\n","            self.authenticate()\n","\n","    def authenticate(self):\n","        auth.authenticate_user()\n","        gauth = GoogleAuth()\n","        gauth.credentials = GoogleCredentials.get_application_default()\n","        self.drive = GoogleDrive(gauth)\n","\n","    def download_file(self, file_id, file_dst):\n","        if self.use_pydrive:\n","            downloaded = self.drive.CreateFile({'id':file_id})\n","            downloaded.FetchMetadata(fetch_all=True)\n","            downloaded.GetContentFile(file_dst)\n","        else:\n","            !gdown --id $file_id -O $file_dst\n","\n","downloader = Downloader(download_with_pydrive)"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"OArDkm_24w4L","executionInfo":{"status":"ok","timestamp":1698231824681,"user_tz":-480,"elapsed":4056,"user":{"displayName":"ziheng jiang","userId":"11981944010465529762"}}},"source":["#@title Imports\n","\n","import clip\n","import os\n","from torch import nn\n","import numpy as np\n","import torch\n","import torch.nn.functional as nnf\n","import sys\n","from typing import Tuple, List, Union, Optional\n","from transformers import GPT2Tokenizer, GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n","from tqdm import tqdm, trange\n","from google.colab import files\n","import skimage.io as io\n","import PIL.Image\n","from IPython.display import Image\n","\n","\n","N = type(None)\n","V = np.array\n","ARRAY = np.ndarray\n","ARRAYS = Union[Tuple[ARRAY, ...], List[ARRAY]]\n","VS = Union[Tuple[V, ...], List[V]]\n","VN = Union[V, N]\n","VNS = Union[VS, N]\n","T = torch.Tensor\n","TS = Union[Tuple[T, ...], List[T]]\n","TN = Optional[T]\n","TNS = Union[Tuple[TN, ...], List[TN]]\n","TSN = Optional[TS]\n","TA = Union[T, ARRAY]\n","\n","\n","D = torch.device\n","CPU = torch.device('cpu')\n","\n","\n","def get_device(device_id: int) -> D:\n","    if not torch.cuda.is_available():\n","        return CPU\n","    device_id = min(torch.cuda.device_count() - 1, device_id)\n","    return torch.device(f'cuda:{device_id}')\n","\n","\n","CUDA = get_device\n","\n","current_directory = os.getcwd()\n","save_path = os.path.join(os.path.dirname(current_directory), \"pretrained_models\")\n","os.makedirs(save_path, exist_ok=True)\n","model_path = os.path.join(save_path, 'model_wieghts.pt')\n"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"4ClW2ebek8DK","executionInfo":{"status":"ok","timestamp":1698231824682,"user_tz":-480,"elapsed":11,"user":{"displayName":"ziheng jiang","userId":"11981944010465529762"}}},"source":["#@title Model\n","\n","class MLP(nn.Module):\n","\n","    def forward(self, x: T) -> T:\n","        return self.model(x)\n","\n","    def __init__(self, sizes: Tuple[int, ...], bias=True, act=nn.Tanh):\n","        super(MLP, self).__init__()\n","        layers = []\n","        for i in range(len(sizes) -1):\n","            layers.append(nn.Linear(sizes[i], sizes[i + 1], bias=bias))\n","            if i < len(sizes) - 2:\n","                layers.append(act())\n","        self.model = nn.Sequential(*layers)\n","\n","\n","class ClipCaptionModel(nn.Module):\n","\n","    #@functools.lru_cache #FIXME\n","    def get_dummy_token(self, batch_size: int, device: D) -> T:\n","        return torch.zeros(batch_size, self.prefix_length, dtype=torch.int64, device=device)\n","\n","    def forward(self, tokens: T, prefix: T, mask: Optional[T] = None, labels: Optional[T] = None):\n","        embedding_text = self.gpt.transformer.wte(tokens)\n","        prefix_projections = self.clip_project(prefix).view(-1, self.prefix_length, self.gpt_embedding_size)\n","        #print(embedding_text.size()) #torch.Size([5, 67, 768])\n","        #print(prefix_projections.size()) #torch.Size([5, 1, 768])\n","        embedding_cat = torch.cat((prefix_projections, embedding_text), dim=1)\n","        if labels is not None:\n","            dummy_token = self.get_dummy_token(tokens.shape[0], tokens.device)\n","            labels = torch.cat((dummy_token, tokens), dim=1)\n","        out = self.gpt(inputs_embeds=embedding_cat, labels=labels, attention_mask=mask)\n","        return out\n","\n","    def __init__(self, prefix_length: int, prefix_size: int = 512):\n","        super(ClipCaptionModel, self).__init__()\n","        self.prefix_length = prefix_length\n","        self.gpt = GPT2LMHeadModel.from_pretrained('gpt2')\n","        self.gpt_embedding_size = self.gpt.transformer.wte.weight.shape[1]\n","        if prefix_length > 10:  # not enough memory\n","            self.clip_project = nn.Linear(prefix_size, self.gpt_embedding_size * prefix_length)\n","        else:\n","            self.clip_project = MLP((prefix_size, (self.gpt_embedding_size * prefix_length) // 2, self.gpt_embedding_size * prefix_length))\n","\n","\n","class ClipCaptionPrefix(ClipCaptionModel):\n","\n","    def parameters(self, recurse: bool = True):\n","        return self.clip_project.parameters()\n","\n","    def train(self, mode: bool = True):\n","        super(ClipCaptionPrefix, self).train(mode)\n","        self.gpt.eval()\n","        return self"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"V7xocT3TUgey","executionInfo":{"status":"ok","timestamp":1698231824682,"user_tz":-480,"elapsed":9,"user":{"displayName":"ziheng jiang","userId":"11981944010465529762"}}},"source":["#@title Caption prediction\n","\n","def generate_beam(model, tokenizer, beam_size: int = 5, prompt=None, embed=None,\n","                  entry_length=67, temperature=1., stop_token: str = '.'):\n","\n","    model.eval()\n","    stop_token_index = tokenizer.encode(stop_token)[0]\n","    tokens = None\n","    scores = None\n","    device = next(model.parameters()).device\n","    seq_lengths = torch.ones(beam_size, device=device)\n","    is_stopped = torch.zeros(beam_size, device=device, dtype=torch.bool)\n","    with torch.no_grad():\n","        if embed is not None:\n","            generated = embed\n","        else:\n","            if tokens is None:\n","                tokens = torch.tensor(tokenizer.encode(prompt))\n","                tokens = tokens.unsqueeze(0).to(device)\n","                generated = model.gpt.transformer.wte(tokens)\n","        for i in range(entry_length):\n","            outputs = model.gpt(inputs_embeds=generated)\n","            logits = outputs.logits\n","            logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n","            logits = logits.softmax(-1).log()\n","            if scores is None:\n","                scores, next_tokens = logits.topk(beam_size, -1)\n","                generated = generated.expand(beam_size, *generated.shape[1:])\n","                next_tokens, scores = next_tokens.permute(1, 0), scores.squeeze(0)\n","                if tokens is None:\n","                    tokens = next_tokens\n","                else:\n","                    tokens = tokens.expand(beam_size, *tokens.shape[1:])\n","                    tokens = torch.cat((tokens, next_tokens), dim=1)\n","            else:\n","                logits[is_stopped] = -float(np.inf)\n","                logits[is_stopped, 0] = 0\n","                scores_sum = scores[:, None] + logits\n","                seq_lengths[~is_stopped] += 1\n","                scores_sum_average = scores_sum / seq_lengths[:, None]\n","                scores_sum_average, next_tokens = scores_sum_average.view(-1).topk(beam_size, -1)\n","                next_tokens_source = next_tokens // scores_sum.shape[1]\n","                seq_lengths = seq_lengths[next_tokens_source]\n","                next_tokens = next_tokens % scores_sum.shape[1]\n","                next_tokens = next_tokens.unsqueeze(1)\n","                tokens = tokens[next_tokens_source]\n","                tokens = torch.cat((tokens, next_tokens), dim=1)\n","                generated = generated[next_tokens_source]\n","                scores = scores_sum_average * seq_lengths\n","                is_stopped = is_stopped[next_tokens_source]\n","            next_token_embed = model.gpt.transformer.wte(next_tokens.squeeze()).view(generated.shape[0], 1, -1)\n","            generated = torch.cat((generated, next_token_embed), dim=1)\n","            is_stopped = is_stopped + next_tokens.eq(stop_token_index).squeeze()\n","            if is_stopped.all():\n","                break\n","    scores = scores / seq_lengths\n","    output_list = tokens.cpu().numpy()\n","    output_texts = [tokenizer.decode(output[:int(length)]) for output, length in zip(output_list, seq_lengths)]\n","    order = scores.argsort(descending=True)\n","    output_texts = [output_texts[i] for i in order]\n","    return output_texts\n","\n","\n","def generate2(\n","        model,\n","        tokenizer,\n","        tokens=None,\n","        prompt=None,\n","        embed=None,\n","        entry_count=1,\n","        entry_length=67,  # maximum number of words\n","        top_p=0.8,\n","        temperature=1.,\n","        stop_token: str = '.',\n","):\n","    model.eval()\n","    generated_num = 0\n","    generated_list = []\n","    stop_token_index = tokenizer.encode(stop_token)[0]\n","    filter_value = -float(\"Inf\")\n","    device = next(model.parameters()).device\n","\n","    with torch.no_grad():\n","\n","        for entry_idx in trange(entry_count):\n","            if embed is not None:\n","                generated = embed\n","            else:\n","                if tokens is None:\n","                    tokens = torch.tensor(tokenizer.encode(prompt))\n","                    tokens = tokens.unsqueeze(0).to(device)\n","\n","                generated = model.gpt.transformer.wte(tokens)\n","\n","            for i in range(entry_length):\n","\n","                outputs = model.gpt(inputs_embeds=generated)\n","                logits = outputs.logits\n","                logits = logits[:, -1, :] / (temperature if temperature > 0 else 1.0)\n","                sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n","                cumulative_probs = torch.cumsum(nnf.softmax(sorted_logits, dim=-1), dim=-1)\n","                sorted_indices_to_remove = cumulative_probs > top_p\n","                sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[\n","                                                    ..., :-1\n","                                                    ].clone()\n","                sorted_indices_to_remove[..., 0] = 0\n","\n","                indices_to_remove = sorted_indices[sorted_indices_to_remove]\n","                logits[:, indices_to_remove] = filter_value\n","                next_token = torch.argmax(logits, -1).unsqueeze(0)\n","                next_token_embed = model.gpt.transformer.wte(next_token)\n","                if tokens is None:\n","                    tokens = next_token\n","                else:\n","                    tokens = torch.cat((tokens, next_token), dim=1)\n","                generated = torch.cat((generated, next_token_embed), dim=1)\n","                if stop_token_index == next_token.item():\n","                    break\n","\n","            output_list = list(tokens.squeeze().cpu().numpy())\n","            output_text = tokenizer.decode(output_list)\n","            generated_list.append(output_text)\n","\n","    return generated_list[0]"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"xE-uUStuv1Nl","executionInfo":{"status":"ok","timestamp":1698231873550,"user_tz":-480,"elapsed":48876,"user":{"displayName":"ziheng jiang","userId":"11981944010465529762"}}},"source":["#@title Choose pretrained model - COCO or Coneptual captions\n","\n","\n","pretrained_model = 'COCO'  # @param ['COCO', 'Conceptual captions']\n","\n","if pretrained_model == 'Conceptual captions':\n","  downloader.download_file(\"14pXWwB4Zm82rsDdvbGguLfx9F8aM7ovT\", model_path)\n","else:\n","  downloader.download_file(\"1IdaBtMSvtyzF0ByVaBHtvM0JYSXRExRX\", model_path)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"7lCgFHSgr_ny","executionInfo":{"status":"ok","timestamp":1698231873551,"user_tz":-480,"elapsed":31,"user":{"displayName":"ziheng jiang","userId":"11981944010465529762"}}},"source":["#@title GPU/CPU\n","\n","\n","is_gpu = True #@param {type:\"boolean\"}\n"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"6bi_2zQ3QD57","executionInfo":{"status":"ok","timestamp":1698231882471,"user_tz":-480,"elapsed":8948,"user":{"displayName":"ziheng jiang","userId":"11981944010465529762"}}},"source":["#@title CLIP model + GPT2 tokenizer\n","\n","device = CUDA(0) if is_gpu else \"cpu\"\n","clip_model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\n","tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"glBzYsgIwhwF","executionInfo":{"status":"ok","timestamp":1698231886268,"user_tz":-480,"elapsed":3828,"user":{"displayName":"ziheng jiang","userId":"11981944010465529762"}}},"source":["#@title Load model weights\n","\n","\n","prefix_length = 10\n","\n","model = ClipCaptionModel(prefix_length)\n","\n","# model.load_state_dict(torch.load(model_path, map_location=CPUï¼Œ strict=False))\n","model.load_state_dict(torch.load(model_path, map_location=CPU), strict=False)\n","\n","model = model.eval()\n","device = CUDA(0) if is_gpu else \"cpu\"\n","model = model.to(device)\n"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":57},"id":"m5jPDsEA5Kub","outputId":"0226a9d2-aeed-4ca7-a01d-692a81113e76"},"source":["#@title Upload Image\n","\n","\n","uploaded = files.upload()\n","\n","if not uploaded:\n","  UPLOADED_FILE = ''\n","elif len(uploaded) == 1:\n","  UPLOADED_FILE = list(uploaded.keys())[0]\n","else:\n","  raise AssertionError('Please upload one image at a time')\n","\n","print(UPLOADED_FILE)"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-d15cf0bd-00f4-4d98-9060-0dd85e8cf16d\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-d15cf0bd-00f4-4d98-9060-0dd85e8cf16d\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"pohtQ8AfWNk_","cellView":"form","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1698231984984,"user_tz":-480,"elapsed":1572,"user":{"displayName":"ziheng jiang","userId":"11981944010465529762"}},"outputId":"e463119f-710d-4092-8280-9867b0d9608f"},"source":["#@title Or download random samples form COCO test set (Karpathy et al. split)\n","\n","IMAGE_NAME = '334321'  # @param ['562207', '579664', '060623', '165547', '334321', '483108', '386164', '354533']\n","\n","name_ = \"COCO_val2014_000000\" + IMAGE_NAME + \".jpg\"\n","images_path = os.path.join(os.path.dirname(current_directory), \"images\")\n","os.makedirs(images_path, exist_ok=True)\n","UPLOADED_FILE = os.path.join(images_path, name_)\n","\n","if not os.path.isfile(UPLOADED_FILE):\n","  download_path = os.path.join(images_path, \"images.zip\")\n","  downloader.download_file(\"1BwJeBME-dpwcCT8IXYeWz7uaPkbexjNB\", download_path)\n","\n","  !unzip {download_path} -d {images_path}\n","\n"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["Archive:  /images/images.zip\n","  inflating: /images/COCO_val2014_000000060623.jpg  \n","  inflating: /images/__MACOSX/._COCO_val2014_000000060623.jpg  \n","  inflating: /images/COCO_val2014_000000165547.jpg  \n","  inflating: /images/__MACOSX/._COCO_val2014_000000165547.jpg  \n","  inflating: /images/COCO_val2014_000000334321.jpg  \n","  inflating: /images/__MACOSX/._COCO_val2014_000000334321.jpg  \n","  inflating: /images/COCO_val2014_000000354533.jpg  \n","  inflating: /images/__MACOSX/._COCO_val2014_000000354533.jpg  \n","  inflating: /images/COCO_val2014_000000386164.jpg  \n","  inflating: /images/__MACOSX/._COCO_val2014_000000386164.jpg  \n","  inflating: /images/COCO_val2014_000000483108.jpg  \n","  inflating: /images/__MACOSX/._COCO_val2014_000000483108.jpg  \n","  inflating: /images/COCO_val2014_000000562207.jpg  \n","  inflating: /images/__MACOSX/._COCO_val2014_000000562207.jpg  \n","  inflating: /images/COCO_val2014_000000579664.jpg  \n","  inflating: /images/__MACOSX/._COCO_val2014_000000579664.jpg  \n"]}]},{"cell_type":"markdown","metadata":{"id":"XyVkuZ07llSC"},"source":["Conceptual captions examples:\n","https://drive.google.com/file/d/1mzH3b0LQrGEWjEva4hI6HE_fIYRIgtBT/view?usp=sharing"]},{"cell_type":"code","metadata":{"id":"rRmcYnEfSMc_"},"source":["#@title Inference\n","use_beam_search = False #@param {type:\"boolean\"}\n","\n","image = io.imread(UPLOADED_FILE)\n","pil_image = PIL.Image.fromarray(image)\n","#pil_img = Image(filename=UPLOADED_FILE)\n","display(pil_image)\n","\n","image = preprocess(pil_image).unsqueeze(0).to(device)\n","with torch.no_grad():\n","    # if type(model) is ClipCaptionE2E:\n","    #     prefix_embed = model.forward_image(image)\n","    # else:\n","    prefix = clip_model.encode_image(image).to(device, dtype=torch.float32)\n","    prefix_embed = model.clip_project(prefix).reshape(1, prefix_length, -1)\n","if use_beam_search:\n","    generated_text_prefix = generate_beam(model, tokenizer, embed=prefix_embed)[0]\n","\n","else:\n","    generated_text_prefix = generate2(model, tokenizer, embed=prefix_embed)\n","print(use_beam_search)\n","\n","print('\\n')\n","print(generated_text_prefix)"],"execution_count":null,"outputs":[]}]}